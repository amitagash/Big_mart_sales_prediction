{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigMart Sales Prediction: End-to-End Pipeline\n",
    "\n",
    "This notebook contains the complete workflow for predicting BigMart sales, ensuring a reproducible flow from raw data to final submission. \n",
    "It uses a **Neural Network (Keras)** which was found to be the best performing model (RMSE ~1015).\n",
    "**Configuration**: No L2 Regularization, No Target Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "RAW_DIR = 'dataset/raw'\n",
    "TRAIN_PATH = os.path.join(RAW_DIR, 'train_v9rqX0R.csv')\n",
    "TEST_PATH = os.path.join(RAW_DIR, 'test_AbJTz2l.csv')\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_raw = pd.read_csv(TRAIN_PATH)\n",
    "test_raw = pd.read_csv(TEST_PATH)\n",
    "print(f\"Train Shape: {train_raw.shape}\")\n",
    "print(f\"Test Shape: {test_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "- **Item_Weight**: Impute missing values with median per `Item_Identifier`.\n",
    "- **Outlet_Size**: Impute missing values using a Random Forest Classifier.\n",
    "- **Item_Fat_Content**: Standardize categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(train_df, test_df):\n",
    "    print(\"--- Starting Data Cleaning ---\")\n",
    "    combined = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    # 1. Impute Item_Weight (Median by Item_Identifier)\n",
    "    item_weight_median = combined.groupby('Item_Identifier')['Item_Weight'].median()\n",
    "    def impute_weight(row):\n",
    "        if pd.isnull(row['Item_Weight']):\n",
    "            return item_weight_median.get(row['Item_Identifier'], np.nan)\n",
    "        return row['Item_Weight']\n",
    "    \n",
    "    combined['Item_Weight'] = combined.apply(impute_weight, axis=1)\n",
    "    combined['Item_Weight'].fillna(combined['Item_Weight'].median(), inplace=True) # Fallback\n",
    "\n",
    "    # 2. Impute Outlet_Size (Random Forest)\n",
    "    # Encode features for RF\n",
    "    impute_df = combined.copy()\n",
    "    features = ['Outlet_Type', 'Outlet_Location_Type', 'Outlet_Establishment_Year']\n",
    "    target = 'Outlet_Size'\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    for col in features:\n",
    "        impute_df[col] = le.fit_transform(impute_df[col].astype(str))\n",
    "        \n",
    "    known = impute_df[impute_df[target].notnull()]\n",
    "    unknown = impute_df[impute_df[target].isnull()]\n",
    "    \n",
    "    if len(unknown) > 0:\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf.fit(known[features], known[target])\n",
    "        predicted_sizes = rf.predict(unknown[features])\n",
    "        combined.loc[combined[target].isnull(), target] = predicted_sizes\n",
    "        print(\"Outlet_Size imputed using Random Forest.\")\n",
    "\n",
    "    # 3. Standardize Item_Fat_Content\n",
    "    mapping = {'LF': 'Low Fat', 'low fat': 'Low Fat', 'reg': 'Regular'}\n",
    "    combined['Item_Fat_Content'] = combined['Item_Fat_Content'].replace(mapping)\n",
    "    \n",
    "    print(\"--- Data Cleaning Completed ---\")\n",
    "    return combined\n",
    "\n",
    "cleaned_df = clean_data(train_raw, test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "- **Item_Visibility_MeanRatio**: Ratio of visibility to mean visibility of item.\n",
    "- **Item_Visibility_Ratio_OutletSize**: Ratio of visibility to mean visibility of outlet size.\n",
    "- **Outlet_Years**: 2013 - Establishment Year.\n",
    "- **Encoding**: Manual Ordinal for sizes/locations, One-Hot for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    print(\"--- Starting Feature Engineering ---\")\n",
    "    \n",
    "    # 1. Item_Visibility_MeanRatio\n",
    "    visibility_avg = df.pivot_table(values='Item_Visibility', index='Item_Identifier')\n",
    "    # First impute 0 visibility\n",
    "    def impute_visibility(row):\n",
    "        if row['Item_Visibility'] == 0:\n",
    "            return visibility_avg.loc[row['Item_Identifier'], 'Item_Visibility']\n",
    "        return row['Item_Visibility']\n",
    "    \n",
    "    df['Item_Visibility'] = df.apply(impute_visibility, axis=1)\n",
    "    \n",
    "    # Re-calc average and create ratio\n",
    "    visibility_avg = df.pivot_table(values='Item_Visibility', index='Item_Identifier')\n",
    "    df['Item_Visibility_MeanRatio'] = df.apply(\n",
    "        lambda x: x['Item_Visibility'] / visibility_avg.loc[x['Item_Identifier'], 'Item_Visibility'], axis=1\n",
    "    )\n",
    "\n",
    "    # 2. Item_Visibility_Ratio_OutletSize\n",
    "    vis_size_avg = df.pivot_table(values='Item_Visibility', index='Outlet_Size')\n",
    "    def get_vis_size_ratio(row):\n",
    "        size = row['Outlet_Size']\n",
    "        vis = row['Item_Visibility']\n",
    "        if size in vis_size_avg.index:\n",
    "             mean_vis = vis_size_avg.loc[size, 'Item_Visibility']\n",
    "             return vis / mean_vis if mean_vis != 0 else 0\n",
    "        return 1.0\n",
    "    \n",
    "    df['Item_Visibility_Ratio_OutletSize'] = df.apply(get_vis_size_ratio, axis=1)\n",
    "\n",
    "    # 3. Outlet_Years\n",
    "    df['Outlet_Years'] = 2013 - df['Outlet_Establishment_Year']\n",
    "    \n",
    "    # 4. Item_Type_Combined\n",
    "    df['Item_Type_Combined'] = df['Item_Identifier'].apply(lambda x: x[0:2])\n",
    "    df['Item_Type_Combined'] = df['Item_Type_Combined'].map(\n",
    "        {'FD': 'Food', 'NC': 'Non-Consumable', 'DR': 'Drinks'}\n",
    "    )\n",
    "    \n",
    "    # 5. Encoding\n",
    "    # Ordinal\n",
    "    size_map = {'Small': 0, 'Medium': 1, 'High': 2}\n",
    "    df['Outlet_Size'] = df['Outlet_Size'].map(size_map)\n",
    "    \n",
    "    loc_map = {'Tier 1': 0, 'Tier 2': 1, 'Tier 3': 2}\n",
    "    df['Outlet_Location_Type'] = df['Outlet_Location_Type'].map(loc_map)\n",
    "    \n",
    "    # One-Hot\n",
    "    df = pd.get_dummies(df, columns=['Item_Fat_Content', 'Outlet_Type', 'Item_Type_Combined'])\n",
    "    \n",
    "    # 6. Drop Redundant\n",
    "    df.drop(columns=['Outlet_Establishment_Year'], inplace=True)\n",
    "    \n",
    "    print(\"--- Feature Engineering Completed ---\")\n",
    "    return df\n",
    "\n",
    "processed_df = engineer_features(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing\n",
    "- Split Train/Test.\n",
    "- Scale Features (RobustScaler).\n",
    "- **NO Target Scaling** (Model trained on raw sales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Train/Test\n",
    "train_len = len(train_raw)\n",
    "train_final = processed_df.iloc[:train_len].copy()\n",
    "test_final = processed_df.iloc[train_len:].copy()\n",
    "\n",
    "# Features & Target\n",
    "X = train_final.drop(columns=['Item_Outlet_Sales'])\n",
    "y = train_final['Item_Outlet_Sales']\n",
    "X_test = test_final.drop(columns=['Item_Outlet_Sales'], errors='ignore')\n",
    "\n",
    "# Drop IDs for training\n",
    "cols_to_drop = ['Item_Identifier', 'Outlet_Identifier', 'Item_Type']\n",
    "X_train_data = X.drop(columns=cols_to_drop)\n",
    "X_test_data = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "# Scaling\n",
    "scaler_x = RobustScaler()\n",
    "X_scaled = scaler_x.fit_transform(X_train_data)\n",
    "X_test_scaled = scaler_x.transform(X_test_data)\n",
    "\n",
    "# Target is NOT scaled for this best performing model\n",
    "y_train = y.values\n",
    "\n",
    "# Validation Split\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_scaled, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training Data Shape: {X_train_split.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "model = build_model(X_train_split.shape[1])\n",
    "history = model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    validation_data=(X_val_split, y_val_split),\n",
    "    batch_size=32,\n",
    "    epochs=150,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# RMSE\n",
    "y_pred = model.predict(X_val_split).flatten()\n",
    "# No inverse transform needed for target\n",
    "rmse = np.sqrt(mean_squared_error(y_val_split, y_pred))\n",
    "print(f\"Validation RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Submission\n",
    "Retraining on full data and generating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retraining on full dataset...\")\n",
    "final_model = build_model(X_scaled.shape[1])\n",
    "final_model.fit(\n",
    "    X_scaled, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=150,\n",
    "    verbose=1,\n",
    "    callbacks=[keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5)]\n",
    ")\n",
    "\n",
    "predictions = final_model.predict(X_test_scaled).flatten()\n",
    "predictions = np.maximum(predictions, 0)\n",
    "\n",
    "submission = test_raw[['Item_Identifier', 'Outlet_Identifier']]\n",
    "submission['Item_Outlet_Sales'] = predictions\n",
    "submission.to_csv('final_submission.csv', index=False)\n",
    "print(\"Submission saved to final_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
