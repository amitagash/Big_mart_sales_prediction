{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# BigMart Sales Prediction - Data Cleaning & EDA\n",
                "\n",
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "\n",
                "%matplotlib inline\n",
                "sns.set_style('whitegrid')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_path = '../dataset/raw/train_v9rqX0R.csv'\n",
                "test_path = '../dataset/raw/test_AbJTz2l.csv'\n",
                "\n",
                "train_df = pd.read_csv(train_path)\n",
                "test_df = pd.read_csv(test_path)\n",
                "\n",
                "print(\"Train Shape:\", train_df.shape)\n",
                "print(\"Test Shape:\", test_df.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Inspection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df.isnull().sum()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Handling Missing Information\n",
                "\n",
                "### Item_Weight Analysis\n",
                "1. Identify Item_Weight mismatches (deviations from median).\n",
                "2. Impute missing Item_Weight using the median of the specific Item_Identifier."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine data for unified analysis\n",
                "combined = pd.concat([train_df, test_df], ignore_index=True)\n",
                "\n",
                "# Calculate median weight per item\n",
                "item_weight_median = combined.groupby('Item_Identifier')['Item_Weight'].median()\n",
                "\n",
                "# Check for mismatches (values that exist but differ from median)\n",
                "def check_mismatch(row, median_map):\n",
                "    if pd.notnull(row['Item_Weight']):\n",
                "        median = median_map.get(row['Item_Identifier'])\n",
                "        if pd.notnull(median) and abs(row['Item_Weight'] - median) > 0.1: # Allow small float diff\n",
                "            return True\n",
                "    return False\n",
                "\n",
                "mismatches = combined.apply(lambda x: check_mismatch(x, item_weight_median), axis=1)\n",
                "print(f\"Number of Item_Weight records deviating from item median: {mismatches.sum()}\")\n",
                "\n",
                "if mismatches.sum() > 0:\n",
                "    print(\"Example Mismatches:\")\n",
                "    print(combined[mismatches][['Item_Identifier', 'Item_Weight']].head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Impute ONLY missing values using the median\n",
                "def impute_weight(row, median_map):\n",
                "    if pd.isnull(row['Item_Weight']):\n",
                "        return median_map.get(row['Item_Identifier'], np.nan)\n",
                "    return row['Item_Weight']\n",
                "\n",
                "train_df['Item_Weight'] = train_df.apply(lambda x: impute_weight(x, item_weight_median), axis=1)\n",
                "test_df['Item_Weight'] = test_df.apply(lambda x: impute_weight(x, item_weight_median), axis=1)\n",
                "\n",
                "# Fallback for any still missing\n",
                "global_median = combined['Item_Weight'].median()\n",
                "train_df['Item_Weight'].fillna(global_median, inplace=True)\n",
                "test_df['Item_Weight'].fillna(global_median, inplace=True)\n",
                "\n",
                "print(\"Missing Item_Weight in Train after imputation:\", train_df['Item_Weight'].isnull().sum())\n",
                "print(\"Missing Item_Weight in Test after imputation:\", test_df['Item_Weight'].isnull().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Outlet_Size Imputation with Random Forest\n",
                "Using `Outlet_Type`, `Outlet_Location_Type`, and `Outlet_Establishment_Year` to predict `Outlet_Size`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare data for imputation model\n",
                "# Combine train and test again to use all available data for training the imputer\n",
                "impute_df = pd.concat([train_df, test_df], ignore_index=True)\n",
                "\n",
                "# Features for prediction\n",
                "features = ['Outlet_Type', 'Outlet_Location_Type', 'Outlet_Establishment_Year']\n",
                "target = 'Outlet_Size'\n",
                "\n",
                "# Label Encode categorical features\n",
                "le = LabelEncoder()\n",
                "for col in features:\n",
                "    impute_df[col] = le.fit_transform(impute_df[col].astype(str))\n",
                "\n",
                "# Split into sets with known and unknown Outlet_Size\n",
                "known_size = impute_df[impute_df[target].notnull()]\n",
                "unknown_size = impute_df[impute_df[target].isnull()]\n",
                "\n",
                "print(f\"Training Imputer on {len(known_size)} rows. Predicting for {len(unknown_size)} rows.\")\n",
                "\n",
                "# Train Random Forest Classifier\n",
                "rf_imputer = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "rf_imputer.fit(known_size[features], known_size[target])\n",
                "\n",
                "# Predict\n",
                "predicted_sizes = rf_imputer.predict(unknown_size[features])\n",
                "\n",
                "# Fill missing values in original dataframes\n",
                "def fill_size(df, model, le_encoders, features): # Helper to apply model\n",
                "    # We need to encode the features exactly as trained\n",
                "    # Note: re-encoding here assumes consistency. Better to map.\n",
                "    # For simplicity in this notebook, we'll iterate and update indices.\n",
                "    pass\n",
                "\n",
                "# Update combined dataframe first then split back? Or just fill by index.\n",
                "impute_df.loc[impute_df[target].isnull(), target] = predicted_sizes\n",
                "\n",
                "# Split back to train and test\n",
                "train_df['Outlet_Size'] = impute_df.loc[:len(train_df)-1, 'Outlet_Size']\n",
                "test_df['Outlet_Size'] = impute_df.loc[len(train_df):, 'Outlet_Size'].values\n",
                "\n",
                "print(\"Missing Outlet_Size in Train:\", train_df['Outlet_Size'].isnull().sum())\n",
                "print(\"Missing Outlet_Size in Test:\", test_df['Outlet_Size'].isnull().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Cleaning Categorical Inconsistencies\n",
                "\n",
                "### Item_Fat_Content\n",
                "Standardizing values: 'LF', 'low fat' -> 'Low Fat', and 'reg' -> 'Regular'."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Original Item_Fat_Content Categories:\", train_df['Item_Fat_Content'].unique())\n",
                "\n",
                "mapping = {'LF': 'Low Fat', 'low fat': 'Low Fat', 'reg': 'Regular'}\n",
                "train_df['Item_Fat_Content'] = train_df['Item_Fat_Content'].replace(mapping)\n",
                "test_df['Item_Fat_Content'] = test_df['Item_Fat_Content'].replace(mapping)\n",
                "\n",
                "print(\"Standardized Item_Fat_Content Categories:\", train_df['Item_Fat_Content'].unique())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Exploratory Data Analysis (EDA)\n",
                "\n",
                "### Univariate Analysis\n",
                "Analyzing the distribution of the target variable `Item_Outlet_Sales` and other independent features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10,6))\n",
                "sns.distplot(train_df['Item_Outlet_Sales'])\n",
                "plt.title('Distribution of Item Outlet Sales')\n",
                "plt.xlabel('Item Outlet Sales')\n",
                "plt.ylabel('Density')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10,6))\n",
                "sns.countplot(x='Item_Fat_Content', data=train_df)\n",
                "plt.title('Count of Item Fat Content')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(15,6))\n",
                "sns.countplot(x='Item_Type', data=train_df)\n",
                "plt.xticks(rotation=90)\n",
                "plt.title('Count of Item Type')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10,6))\n",
                "sns.countplot(x='Outlet_Size', data=train_df)\n",
                "plt.title('Count of Outlet Size')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Bivariate Analysis\n",
                "Analyzing relationships between features and the target variable `Item_Outlet_Sales`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10,6))\n",
                "sns.boxplot(x='Item_Fat_Content', y='Item_Outlet_Sales', data=train_df)\n",
                "plt.title('Item Fat Content vs Item Outlet Sales')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(15,8))\n",
                "sns.boxplot(x='Item_Type', y='Item_Outlet_Sales', data=train_df)\n",
                "plt.xticks(rotation=90)\n",
                "plt.title('Item Type vs Item Outlet Sales')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10,6))\n",
                "sns.boxplot(x='Outlet_Size', y='Item_Outlet_Sales', data=train_df)\n",
                "plt.title('Outlet Size vs Item Outlet Sales')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10,6))\n",
                "sns.scatterplot(x='Item_MRP', y='Item_Outlet_Sales', data=train_df)\n",
                "plt.title('Item MRP vs Item Outlet Sales')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs('../dataset/processed', exist_ok=True)\n",
                "\n",
                "train_df.to_csv('../dataset/processed/cleaned_train.csv', index=False)\n",
                "test_df.to_csv('../dataset/processed/cleaned_test.csv', index=False)\n",
                "\n",
                "print(\"Cleaned datasets saved to ../dataset/processed/\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}